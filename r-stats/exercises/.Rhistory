# What proportion of these 1,000 averages are more than 1 gram away from the average of x ?
set.seed(1)
n<-1000
rnd_1000 <- vector("numeric", n)
for (i in 1:n) {
rnd_1000[i]<-mean(sample(X, 5))
}
rnd_1000>mean(X)
avg_plus1 <- mean(X)+1
mean(abs(rnd_1000)>avg_plus1)
mean(X)
X
rnd_1000>mean(X)
avg_plus1 <- mean(X)+1
mean(rnd_1000 > avg_plus1)
rnd_1000[0]
rnd_1000[1]
avg_plus1[1]
mean(X)
avg_minus1 <- mean(X)+1
mean(rnd_1000 > avg_plus1 || rnd_1000<avg_minus1)
rnd_1000 > avg_plus1
rnd_1000 < avg_minus1
mean(rnd_1000 > avg_plus1)
mean(rnd_1000 < avg_minus1)
avg_minus1 <- mean(X)-1
mean(rnd_1000 > avg_plus1)
mean(rnd_1000 < avg_minus1)
mean(rnd_1000 > avg_plus1 || rnd_1000 < avg_minus1)
mean(rnd_1000>mean(X))
mean(rnd_1000 > avg_plus1) + mean(rnd_1000 < avg_minus1)
# Q2: We are now going to increase the number of times we redo the sample from 1,000 to 10,000. Set the seed at 1, then using a for-loop take a random sample of 5 mice 10,000 times. Save these averages.
# What proportion of these 10,000 averages are more than 1 gram away from the average of x ?
set.seed(1)
n<-10000
sample10000_avg <- vector("numeric", n)
for (i in 1:n) {
sample10000_avg[i]<-mean(sample(X, 5))
}
mean(abs(sample10000_avg - mean(X)) > 1)
install.packages("gapminder")
library(gapminder)
data(gapminder)
head(gapminder)
class(gapminder)
# Create a vector x of the life expectancies of each country for the year 1952.
x <- gapminder$lifeExp
x
# Create a vector x of the life expectancies of each country for the year 1952.
x <- gapminder$lifeExp %>% unlist
# Create a vector x of the life expectancies of each country for the year 1952.
x <- unlist(gapminder$lifeExp)
x
# Create a vector x of the life expectancies of each country for the year 1952.
x <- gapminder$lifeExp
class(x)
head(gapminder)
# Create a vector x of the life expectancies of each country for the year 1952.
x <- filter(gapminder, year==1952) #  gapminder$lifeExp
x
# Create a vector x of the life expectancies of each country for the year 1952.
x <- filter(gapminder, year==1952) #  gapminder$lifeExp
# Create a vector x of the life expectancies of each country for the year 1952.
x <- filter(gapminder, gapminder$year==1952) #  gapminder$lifeExp
x
# Create a vector x of the life expectancies of each country for the year 1952.
x <- filter(gapminder, year=="1952") #  gapminder$lifeExp
data(gapminder)
head(gapminder)
class(gapminder)
df<-data(gapminder)
class(df)
df<-data(gapminder)
head(gapminder)
class(df)
class(gapminder)
# Create a vector x of the life expectancies of each country for the year 1952.
x <- filter(gapminder, year==1952) #  gapminder$lifeExp
filter(gapminder, year==1952)
gapminder$year
filter(gapminder, gapminder$year==1952)
filter(gapminder, gapminder$year=1952)
filter(gapminder, gapminder$year=="1952")
filter(gapminder$year=="1952")
filter(gapminder, year==1952)
set.seed(1)
Ns <- seq(5,45,5)
library(rafalib)
mypar(3,3)
for(N in Ns){
medians <- replicate(10000, median ( rnorm(N) ) )
title <- paste("N=",N,", avg=",round( mean(medians), 2) , ", sd*sqrt(N)=", round( sd(medians)*sqrt(N),2) )
qqnorm(medians, main = title )
qqline(medians)
}
# Q7: We can derive approximation of the distribution of the sample average or the t-statistic theoretically. However, suppose we are interested in the distribution of a statistic for which a theoretical approximation is not immediately obvious.
# Consider the sample median as an example. Use a Monte Carlo to determine which of the following best approximates the median of a sample taken from normally distributed population with mean 0 and standard deviation 1.
# A) Just like for the average, the sample median is approximately normal with mean 0 and SD 1/rt(N).
# B) The sample median is not approximately normal.
# C) The sample median is t-distributed for small samples and normally distributed for large ones.
# D) The sample median is approximately normal with mean 0 and SD larger than 1/rt(N).
B <- 10000
Ns <- seq(5, 30, 5)
ps <- seq(1/(B+1), 1-1/(B+1), len=B)
par(mfrow=c(2,3))
for (n in Ns) {
quantiles <- qt(ps, n-1)
medians <- replicate(B, {
X <- rnorm(n)
median(X)
})
title = paste("N=", n, ", avg=", round( mean(medians), 2 ), ", sd*sqrt(n)=", round( sd(medians)*sqrt(N),2))
qqnorm(medians, main = title)
qqline(medians)
}
library(downloader)
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/babies.txt"
filename <- basename(url)
download(url, destfile=filename)
babies <- read.table("babies.txt", header=TRUE)
bwt.nonsmoke <- filter(babies, smoke==0) %>% select(bwt) %>% unlist
bwt.smoke <- filter(babies, smoke==1) %>% select(bwt) %>% unlist
# Q1: We will generate the following random variable based on a sample size of 10 and observe the following difference:
N=10
library(dplyr)
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/babies.txt"
filename <- basename(url)
download(url, destfile=filename)
babies <- read.table("babies.txt", header=TRUE)
bwt.nonsmoke <- filter(babies, smoke==0) %>% select(bwt) %>% unlist
bwt.smoke <- filter(babies, smoke==1) %>% select(bwt) %>% unlist
# Q1: We will generate the following random variable based on a sample size of 10 and observe the following difference:
N=10
set.seed(1)
nonsmokers <- sample(bwt.nonsmoke , N)
smokers <- sample(bwt.smoke , N)
obs <- mean(smokers) - mean(nonsmokers)
# The question is whether this observed difference is statistically significant. We do not want to rely on the assumptions needed for the normal or t-distribution approximations to hold, so instead we will use permutations. We will reshuffle the data and recompute the mean. We can create one permuted sample with the following code:
dat <- c(smokers, nonsmokers)
dat
shuffle <- sample( dat )
shuffle
# The question is whether this observed difference is statistically significant. We do not want to rely on the assumptions needed for the normal or t-distribution approximations to hold, so instead we will use permutations. We will reshuffle the data and recompute the mean. We can create one permuted sample with the following code:
smokers
smokersstar <- shuffle[1:N]
nonsmokersstar <- shuffle[(N+1):(2*N)]
mean(smokersstar)-mean(nonsmokersstar)
obs
# The last value is one observation from the null distribution we will construct.
# Set the seed at 1, and then repeat the permutation 1,000 times to create a null distribution.
# What is the permutation derived p-value for our observation?
set.seed(1)
B <- 1000
n <- 10
null_mean <- function(n) {
dat <- c(smokers, nonsmokers)
shuffle <- sample( dat )
smokersstar <- shuffle[1:n]
nonsmokersstar <- shuffle[(n+1):(2*n)]
mean(smokersstar)-mean(nonsmokersstar)
}
null_distribution <- replicate(B, null_mean(n))
null_distribution
mean(abs(null_distribution) >obs)
abs(null_distribution) >obs
abs(null_distribution)
obs
mean(abs(null_distribution) >abs(obs))
mean( abs(null_distribution) >= abs(obs) )
mean( abs(null_distribution) >= abs(obs)+1 )
# Their answer:
set.seed(1)
null <- replicate(1000, {
shuffle <- sample( dat )
smokersstar <- shuffle[1:N]
nonsmokersstar <- shuffle[(N+1):(2*N)]
mean(smokersstar)-mean(nonsmokersstar)
})
( sum( abs(null) >= abs(obs)) +1 ) / ( length(null)+1 )
null_mean <- function(n) {
dat <- c(smokers, nonsmokers)
shuffle <- sample( dat )
smokersstar <- shuffle[1:n]
nonsmokersstar <- shuffle[(n+1):(2*n)]
mean(smokersstar)-mean(nonsmokersstar)
}
null_distribution <- replicate(B, null_mean(n))
mean( abs(null_distribution) >= abs(obs) )
# The last value is one observation from the null distribution we will construct.
# Set the seed at 1, and then repeat the permutation 1,000 times to create a null distribution.
# What is the permutation derived p-value for our observation?
set.seed(1)
B <- 1000
n <- 10
null_mean <- function(n) {
dat <- c(smokers, nonsmokers)
shuffle <- sample( dat )
smokersstar <- shuffle[1:n]
nonsmokersstar <- shuffle[(n+1):(2*n)]
mean(smokersstar)-mean(nonsmokersstar)
}
null_distribution <- replicate(B, null_mean(n))
mean( abs(null_distribution) >= abs(obs) )
# The last value is one observation from the null distribution we will construct.
# Set the seed at 1, and then repeat the permutation 1,000 times to create a null distribution.
# What is the permutation derived p-value for our observation?
set.seed(1)
B <- 1000
n <- 10
null_mean <- function(n) {
dat <- c(smokers, nonsmokers)
shuffle <- sample( dat )
smokersstar <- shuffle[1:n]
nonsmokersstar <- shuffle[(n+1):(2*n)]
mean(smokersstar)-mean(nonsmokersstar)
}
null_distribution <- replicate(B, null_mean(n))
mean( abs(null_distribution) >= abs(obs)+1 ) # 0.116
# Q2: Repeat the above exercise, but instead of the differences in mean, consider the differences in median obs <- median(smokers) - median(nonsmokers).
# What is the permutation based p-value?
set.seed(1)
# Observed:
nonsmokers <- sample(bwt.nonsmoke , N)
smokers <- sample(bwt.smoke , N)
obs_median <- median(smokers) - median(nonsmokers)
# Null:
null_median <- function(n) {
dat <- c(smokers, nonsmokers)
shuffle <- sample( dat )
smokersstar <- shuffle[1:n]
nonsmokersstar <- shuffle[(n+1):(2*n)]
mean(smokersstar)-mean(nonsmokersstar)
}
# Q2: Repeat the above exercise, but instead of the differences in mean, consider the differences in median obs <- median(smokers) - median(nonsmokers).
# What is the permutation based p-value?
set.seed(1)
# Observed:
nonsmokers <- sample(bwt.nonsmoke , N)
smokers <- sample(bwt.smoke , N)
obs_median <- median(smokers) - median(nonsmokers)
# Null:
null_median <- function(n) {
dat <- c(smokers, nonsmokers)
shuffle <- sample( dat )
smokersstar <- shuffle[1:n]
nonsmokersstar <- shuffle[(n+1):(2*n)]
median(smokersstar)-median(nonsmokersstar)
}
null_distribution <- replicate(B, null_median(n))
# Q2: Repeat the above exercise, but instead of the differences in mean, consider the differences in median obs <- median(smokers) - median(nonsmokers).
# What is the permutation based p-value?
set.seed(1)
B <- 1000
n <- 10
# Observed:
nonsmokers <- sample(bwt.nonsmoke , N)
smokers <- sample(bwt.smoke , N)
obs_median <- median(smokers) - median(nonsmokers)
# Null:
null_median <- function(n) {
dat <- c(smokers, nonsmokers)
shuffle <- sample( dat )
smokersstar <- shuffle[1:n]
nonsmokersstar <- shuffle[(n+1):(2*n)]
median(smokersstar)-median(nonsmokersstar)
}
null_distribution <- replicate(B, null_median(n))
# pvalue:
mean( abs(null_distribution) >= abs(obs_median) )
d = read.csv("assoctest.csv")
library(downloader)
d = read.csv("assoctest.csv")
d = read.csv("assoctest.csv")
d = read.csv("assoctest.csv")
head(d)
?table
# Q1: Compute the Chi-square test for the association of genotype with case/control status
# (using the table() function and the chisq.test() function). Examine the table to see if it looks enriched for association by eye.
# What is the X-squared statistic?
table(d)
chisq.test(allele_table)
class(allele_table)
# Q1: Compute the Chi-square test for the association of genotype with case/control status
# (using the table() function and the chisq.test() function). Examine the table to see if it looks enriched for association by eye.
# What is the X-squared statistic?
allele_table <- table(d)
class(allele_table)
chisq.test(allele_table)
# Q2: Compute the Fisher's exact test ( fisher.test() ) for the same table. What is the p-value?
fisher.test(allele_table)
data(nym.2002, package="UsingR")
library(dplyr)
dat <- data(nym.2002, package="UsingR")
head(dat)
head(nym.2002)
# Q1: Use dplyr to create two new data frames: males and females, with the data for each gender.
# For males, what is the Pearson correlation between age and time to finish?
males <- filter(nym.2002, gender=="Male")
head(males)
females <- filter(nym.2002, gender=="Female")
cor(males$age, males$time, method="pearson")
plot(males$age, males$time)
par(mfrow=c(1,1))
plot(males$age, males$time)
# Q2: For females, what is the Pearson correlation between age and time to finish?
par(mfrow=c(1,1))
plot(females$age, females$time)
cor(females$age, females$time, method="pearson")
# Q3: If we interpret these correlations without visualizing the data, we would conclude that the older we get, the slower we run marathons, regardless of gender.
# Look at scatterplots and boxplots of times stratified by age groups (20-25, 25-30, etc..).
boxplot(split(females$time, round(females$age)))
# Q3: If we interpret these correlations without visualizing the data, we would conclude that the older we get, the slower we run marathons, regardless of gender.
# Look at scatterplots and boxplots of times stratified by age groups (20-25, 25-30, etc..).
boxplot(split(females$time, seq(min(round(females$age)), max(round(females$age))), 5)
# Q3: If we interpret these correlations without visualizing the data, we would conclude that the older we get, the slower we run marathons, regardless of gender.
# Look at scatterplots and boxplots of times stratified by age groups (20-25, 25-30, etc..).
boxplot(split(females$time, seq(min(round(females$age)), max(round(females$age))), 5))
# Q3: If we interpret these correlations without visualizing the data, we would conclude that the older we get, the slower we run marathons, regardless of gender.
# Look at scatterplots and boxplots of times stratified by age groups (20-25, 25-30, etc..).
boxplot( split( females$time, seq( min( round(females$age) ), max( round(females$age) ), 5) ) )
# Q3: If we interpret these correlations without visualizing the data, we would conclude that the older we get, the slower we run marathons, regardless of gender.
# Look at scatterplots and boxplots of times stratified by age groups (20-25, 25-30, etc..).
boxplot( split( females$time, seq( round( min( females$age) ), round( max( females$age) ), 5) ) )
# Q3: If we interpret these correlations without visualizing the data, we would conclude that the older we get, the slower we run marathons, regardless of gender.
# Look at scatterplots and boxplots of times stratified by age groups (20-25, 25-30, etc..).
boxplot( split( females$time, seq( min( females$age), max( females$age), 5) ) )
# Q3: If we interpret these correlations without visualizing the data, we would conclude that the older we get, the slower we run marathons, regardless of gender.
# Look at scatterplots and boxplots of times stratified by age groups (20-25, 25-30, etc..).
par(mfrow=c(1,2))
boxplot( split( females$time, seq( min( females$age), max( females$age), 5) ), main="Female" )
boxplot( split( males$time, seq( min( males$age), max( males$age), 5) ), main="Male" )
# Q3: If we interpret these correlations without visualizing the data, we would conclude that the older we get, the slower we run marathons, regardless of gender.
# Look at scatterplots and boxplots of times stratified by age groups (20-25, 25-30, etc..).
par(mfrow=c(2,2))
boxplot( split( females$time, seq( min( females$age), max( females$age), 5) ), main="Female" )
boxplot( split( males$time, seq( min( males$age), max( males$age), 5) ), main="Male" )
plot(females$age, females$time, main="Female")
plot(males$age, males$time, main="Male")
# Their code:
library(rafalib)
mypar(2,2)
plot(females$age, females$time)
plot(males$age, males$time)
group <- floor(females$age/5) * 5
boxplot(females$time~group)
group <- floor(males$age/5) * 5
boxplot(males$time~group)
time = sort(nym.2002$time)
# Create a vector time of the sorted times:
time <- sort(nym.2002$time)
# Q1: What is the fastest time divided by the median time?
max(time)/median(time)
# Q1: What is the fastest time divided by the median time?
max(time)
median(time)
head(time)
time
# Q1: What is the fastest time divided by the median time?
max(time)
median(time)
# Q1: What is the fastest time divided by the median time?
min(time) # 566.7833
min(time)/median(time)
# Q2: What is the slowest time divided by the median time?
max(time)/median(time)
library(dslabs)
# The dslabs package has a dataset called divorce_margarine that has data from 2000 to 2009 about the per capita US consumption of margarine and the divorces per 1000 in Maine. Run this example.
data("divorce_margarine")
plot(divorce_margarine$margarine_consumption_per_capita, divorce_margarine$divorce_rate_maine)
cor(divorce_margarine$margarine_consumption_per_capita, divorce_margarine$divorce_rate_maine)
library(dslabs)
# The dslabs package has a dataset called divorce_margarine that has data from 2000 to 2009 about the per capita US consumption of margarine and the divorces per 1000 in Maine. Run this example.
data("divorce_margarine")
install.packages("dslabs")
library(dslabs)
# The dslabs package has a dataset called divorce_margarine that has data from 2000 to 2009 about the per capita US consumption of margarine and the divorces per 1000 in Maine. Run this example.
data("divorce_margarine")
plot(divorce_margarine$margarine_consumption_per_capita, divorce_margarine$divorce_rate_maine)
cor(divorce_margarine$margarine_consumption_per_capita, divorce_margarine$divorce_rate_maine)
par(mfrow=c(1,1))
plot(divorce_margarine$margarine_consumption_per_capita, divorce_margarine$divorce_rate_maine)
cor(divorce_margarine$margarine_consumption_per_capita, divorce_margarine$divorce_rate_maine)
# contains weight of chicks in grams as they grow from day 0 to day 21.
data(ChickWeight)
# This dataset also splits up the chicks by different protein diets, which are coded from 1 to 4.
head(ChickWeight)
plot( ChickWeight$Time, ChickWeight$weight, col=ChickWeight$Diet)
# We use this dataset to also show an important operation in R (not related to robust summaries): reshape.
# the rows here represent time points rather than individuals. To facilitate the comparison of weights at different time points and across the different chicks, we will reshape the data so that each row is a chick.
chick <- reshape(ChickWeight, idvar=c("Chick","Diet"), timevar="Time", direction="wide")
head(chick)
# remove nulls values:
chick = na.omit(chick)
# Q1: Focus on the chick weights on day 4 (check the column names of chick and note the numbers).
# How much does the average of chick weights at day 4 increase if we add an outlier measurement of 3000 grams?
# Specifically, what is the average weight of the day 4 chicks, including the outlier chick, divided by the average of the weight of the day 4 chicks without the outlier.
# Hint: use c() to add a number to a vector.
head(chick)
# Q1: Focus on the chick weights on day 4 (check the column names of chick and note the numbers).
# How much does the average of chick weights at day 4 increase if we add an outlier measurement of 3000 grams?
# Specifically, what is the average weight of the day 4 chicks, including the outlier chick, divided by the average of the weight of the day 4 chicks without the outlier.
# Hint: use c() to add a number to a vector.
head(chick, 20)
# Q1: Focus on the chick weights on day 4 (check the column names of chick and note the numbers).
# How much does the average of chick weights at day 4 increase if we add an outlier measurement of 3000 grams?
# Specifically, what is the average weight of the day 4 chicks, including the outlier chick, divided by the average of the weight of the day 4 chicks without the outlier.
# Hint: use c() to add a number to a vector.
View(chick)
day4_chick <- select(chick, weight.4)
day4_chick
reg_avg <- mean(day4_chick)
class(day4_chick)
day4_chick <- select(chick, weight.4) %>% unlist
reg_avg <- mean(day4_chick)
reg_avg
outlier_avg <- c(day4_chick, 3000)
outlier_chick
outlier_chick <- c(day4_chick, 3000)
outlier_chick
outlier_avg <- mean(outlier_chick)
outlier_avg
outlier_avg/reg_avg
# Q2: In exercise 1, we saw how sensitive the mean is to outliers. Now let's see what happens when we use the median instead of the mean.
# Compute the same ratio, but now using median instead of mean.
# Specifically, what is the median weight of the day 4 chicks, including the outlier chick, divided by the median of the weight of the day 4 chicks without the outlier.
reg_median <- median(day4_chick)
outlier_median <- median(outlier_chick)
outlier_median/reg_median
# Q3: Now try the same thing with the sample standard deviation (the sd() function in R).
# Add a chick with weight 3000 grams to the chick weights from day 4.
# How much does the standard deviation change?
# What's the standard deviation with the outlier chick divided by the standard deviation without the outlier chick?
reg_sd <- sd(day4_chick)
outlier_sd <- sd(outlier_chick)
outlier_sd/reg_sd
# Q4: Compare the result above to the median absolute deviation in R, which is calculated with the mad() function. Note that the MAD is unaffected by the addition of a single outlier.
# The mad() function in R includes the scaling factor 1.4826, such that mad() and sd() are very similar for a sample from a normal distribution.
# What's the MAD with the outlier chick divided by the MAD without the outlier chick?
reg_mad <- mad(day4_chick)
outlier_mad <- mad(outlier_chick)
outlier_mad/reg_mad
# Q5: Our last question relates to how the Pearson correlation is affected by an outlier as compared to the Spearman correlation. The Pearson correlation between x and y is given in R by cor(x,y). The Spearman correlation is given by cor(x,y,method="spearman").
# Plot the weights of chicks from day 4 and day 21. We can see that there is some general trend, with the lower weight chicks on day 4 having low weight again on day 21, and likewise for the high weight chicks.
plot(chick$weight.4, chick$weight.21)
# Calculate the Pearson correlation of the weights of chicks from day 4 and day 21.
corr(chick$weight.4, chick$weight.21)
# Calculate the Pearson correlation of the weights of chicks from day 4 and day 21.
cor(chick$weight.4, chick$weight.21)
# Calculate the Pearson correlation of the weights of chicks from day 4 and day 21.
cor(chick$weight.4, chick$weight.21, method="pearson")
# Now calculate how much the Pearson correlation changes if we add a chick that weighs 3000 on day 4 and 3000 on day 21. Again, divide the Pearson correlation with the outlier chick over the Pearson correlation computed without the outliers.
outlier_pcor <- cor(c(chick$weight.4, 3000), c(chick$weight.21, 3000))
outlier_pcor/reg_pcor
# Calculate the Pearson correlation of the weights of chicks from day 4 and day 21.
reg_pcor <- cor(chick$weight.4, chick$weight.21, method="pearson") #0.4159499
# Now calculate how much the Pearson correlation changes if we add a chick that weighs 3000 on day 4 and 3000 on day 21. Again, divide the Pearson correlation with the outlier chick over the Pearson correlation computed without the outliers.
outlier_pcor <- cor(c(chick$weight.4, 3000), c(chick$weight.21, 3000))
outlier_pcor/reg_pcor
outlier_pcor
# Spearman
reg_scor <- cor(chick$weight.4, chick$weight.21, method="spearman") #0.4159499
reg_scor
# Now calculate how much the Pearson correlation changes if we add a chick that weighs 3000 on day 4 and 3000 on day 21. Again, divide the Pearson correlation with the outlier chick over the Pearson correlation computed without the outliers.
outlier_scor <- cor(c(chick$weight.4, 3000), c(chick$weight.21, 3000), method="spearman") # 0.9861002
outlier_scor
outlier_scor/reg_scor
# contains weight of chicks in grams as they grow from day 0 to day 21.
data(ChickWeight)
# This dataset also splits up the chicks by different protein diets, which are coded from 1 to 4.
head(ChickWeight)
plot( ChickWeight$Time, ChickWeight$weight, col=ChickWeight$Diet)
# We use this dataset to also show an important operation in R (not related to robust summaries): reshape.
# the rows here represent time points rather than individuals. To facilitate the comparison of weights at different time points and across the different chicks, we will reshape the data so that each row is a chick.
#reshape the data from _long_ to _wide_, where the columns Chick and Diet are the ID's and the column Time indicates different observations for each ID.
chick <- reshape(ChickWeight, idvar=c("Chick","Diet"), timevar="Time", direction="wide")
head(chick)
# remove nulls values chicks:
chick <- na.omit(chick)
# Q1: Save the weights of the chicks on day 4 from diet 1 as a vector x.
x <- filter(chick, Diet == "1") %>% select(weight.4) %>% unlist
class(x)
x
# Save the weights of the chicks on day 4 from diet 4 as a vector y.
y <- filter(chick, Diet == "4") %>% select(weight.4) %>% unlist
# Perform a t-test comparing x and y (in R the function t.test(x,y) will perform the test). Then perform a Wilcoxon test of x and y (in R the function wilcox.test(x,y) will perform the test). A warning will appear that an exact p-value cannot be calculated with ties, so an approximation is used, which is fine for our purposes.
t.test(x,y)
# Then perform a Wilcoxon test of x and y (in R the function wilcox.test(x,y) will perform the test). A warning will appear that an exact p-value cannot be calculated with ties, so an approximation is used, which is fine for our purposes.
wilcox.test(x,y)
# Perform a t-test of x and y, after adding a single chick of weight 200 grams to x (the diet 1 chicks).
x_new <- c(x, 200)
# What is the p-value from this test? The p-value of a test is available with the following code: t.test(x,y)$p.value
t.test(x_new,y)$p.value
# Q2: Do the same for the Wilcoxon test. The Wilcoxon test is robust to the outlier. In addition, it has less assumptions that the t-test on the distribution of the underlying data.
wilcox.test(x_new,y)
# Q2: Do the same for the Wilcoxon test. The Wilcoxon test is robust to the outlier. In addition, it has less assumptions that the t-test on the distribution of the underlying data.
wilcox.test(x_new,y)$p.value
# Q2: Do the same for the Wilcoxon test. The Wilcoxon test is robust to the outlier. In addition, it has less assumptions that the t-test on the distribution of the underlying data.
wilcox.test(x_new,y, exact=FALSE)$p.value
# Q3: We will now investigate a possible downside to the Wilcoxon-Mann-Whitney test statistic.
# Using the following code to make three boxplots, showing the true Diet 1 vs 4 weights, and then two altered versions: one with an additional difference of 10 grams and one with an additional difference of 100 grams.
# Use the x and y as defined above, NOT the ones with the added outlier.
library(rafalib)
mypar(1,3)
boxplot(x,y)
boxplot(x,y+10)
boxplot(x,y+100)
# What is the difference in t-test statistic (obtained by t.test(x,y)$statistic) between adding 10 and adding 100 to all the values in the group y?
# Take the the t-test statistic with x and y+10 and subtract the t-test statistic with x and y+100. The value should be positive.
t.test(x,y+100)$statistic - t.test(x,y+10)$statistic
# What is the difference in t-test statistic (obtained by t.test(x,y)$statistic) between adding 10 and adding 100 to all the values in the group y?
# Take the the t-test statistic with x and y+10 and subtract the t-test statistic with x and y+100. The value should be positive.
t.test(x,y+10)$statistic - t.test(x,y+100)$statistic
